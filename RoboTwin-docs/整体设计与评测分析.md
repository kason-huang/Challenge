# RoboTwin 整体设计分析

## 一、项目概览

**RoboTwin 2.0** 是一个可扩展的双臂机器人操作数据生成和基准测试框架，主要用于：
- 生成高质量专家轨迹数据
- 提供标准化评测协议
- 支持跨平台双臂操作

**核心组成：**
1. **RoboTwin-OD**：包含731个实例、147个类别的大型物体数据集
2. **自动化专家数据生成管道**：结合多模态大语言模型（MLLM）与仿真反馈
3. **50个双臂任务基准**：支持5种机器人平台
4. **预收集数据集**：超过100,000条轨迹

---

## 二、整体架构设计

### 2.1 目录结构

```
RoboTwin/
├── envs/              # 环境定义（53个任务）
│   ├── _base_task.py       # 基类，所有任务继承于此
│   ├── _GLOBAL_CONFIGS.py  # 全局配置
│   ├── robot/             # 机器人定义
│   ├── camera/            # 相机配置
│   └── utils/             # 工具函数
├── policy/            # 策略实现（10+种策略）
│   ├── ACT/
│   ├── DP/
│   ├── DP3/
│   ├── RDT/
│   ├── Pi0/
│   ├── Pi05/
│   ├── openvla-oft/
│   ├── TinyVLA/
│   ├── DexVLA/
│   ├── LLaVA-VLA/
│   └── GO1/
├── script/            # 评测和数据收集脚本
│   ├── eval_policy.py           # 标准评测脚本
│   ├── eval_policy_client.py   # 客户端模式评测
│   ├── collect_data.py        # 数据收集
│   └── policy_model_server.py # 模型服务器
├── code_gen/          # 代码生成模块
├── description/       # 物体描述和指令生成
│   ├── objects_description/  # 120个物体描述
│   └── utils/              # 描述生成工具
├── task_config/       # 任务配置
│   ├── _camera_config.yml     # 相机配置
│   ├── _embodiment_config.yml # 机器人配置
│   ├── _eval_step_limit.yml  # 评测步数限制
│   ├── demo_clean.yml       # 清洁环境配置
│   └── demo_randomized.yml # 随机环境配置
└── assets/            # 资源文件（背景纹理等）
```

### 2.2 核心组件

#### 环境基类 (`Base_Task`)

位于 `envs/_base_task.py`，提供统一接口和功能：

```python
class Base_Task(gym.Env):
    def __init__(self):
        # 初始化仿真引擎
        # 加载机器人
        # 加载相机
        # 配置域随机化参数

    def setup_demo(self):      # 设置演示场景
    def play_once(self):       # 执行一次任务
    def check_success(self):   # 检查任务成功（子类实现）
    def get_obs(self):         # 获取观测
    def load_actors(self):     # 加载任务相关物体
    def grasp_actor(self):      # 抓取物体
    def place_actor(self):      # 放置物体
    def check_stable(self):     # 检查场景稳定性
```

#### 任务定义模式

每个任务文件（如 `beat_block_hammer.py`）遵循统一模式：

```python
class beat_block_hammer(Base_Task):
    def setup_demo(self, **kwargs):
        super()._init_task_env_(**kwargs)
        self.load_actors()  # 加载hammer和block

    def load_actors(self):
        # 定义物体位置和约束
        self.add_prohibit_area()  # 设置禁止区域

    def play_once(self):
        # 定义专家动作序列
        self.move(self.grasp_actor(...))
        self.move(self.place_actor(...))
        return self.info

    def check_success(self):
        # 定义成功条件
        hammer_pose = self.hammer.get_functional_point(0, "pose").p
        block_pose = self.block.get_functional_point(1, "pose").p
        eps = np.array([0.02, 0.02])
        return np.all(abs(hammer_pose[:2] - block_pose[:2]) < eps) and \
               self.check_actors_contact(self.hammer.get_name(), self.block.get_name())
```

---

## 三、评测系统详解

### 3.1 评测配置系统

#### 配置文件

`task_config/_eval_step_limit.yml` 定义每个任务的评测步数限制：
```yaml
adjust_bottle: 400
beat_block_hammer: 400
blocks_ranking_rgb: 1200
...
```

#### 评测环境配置

**demo_randomized.yml**（Hard模式）：
```yaml
domain_randomization:
  random_background: true        # 随机背景纹理
  cluttered_table: true          # 添加10个干扰物体
  clean_background_rate: 0.02    # 2%概率使用清洁背景
  random_head_camera_dis: 0
  random_table_height: 0.03     # 随机桌子高度±3cm
  random_light: true              # 随机光照
  crazy_random_light_rate: 0.02 # 2%极端光照
```

**demo_clean.yml**（Easy模式）：
```yaml
domain_randomization:
  random_background: false
  cluttered_table: false
  random_light: false
  ...
```

### 3.2 评测流程

#### 标准评测脚本 (`script/eval_policy.py`)

```python
def main(usr_args):
    # 1. 加载配置
    args = yaml.load(task_config)

    # 2. 设置机器人（支持5种平台）
    embodiment_type = args.get("embodiment")
    # 可选：
    # - [aloha-agilex] 单机器人双臂
    # - [left_robot, right_robot, distance] 双机器人组合

    # 3. 初始化策略
    get_model = eval_function_decorator(policy_name, "get_model")
    model = get_model(usr_args)

    # 4. 运行评测
    st_seed, suc_num = eval_policy(
        TASK_ENV, args, model,
        test_num=100,  # 默认100次测试
        instruction_type=instruction_type
    )

    # 5. 保存结果
    save_result(suc_nums)
```

#### 核心评测循环 (`eval_policy` 函数)

```python
def eval_policy(TASK_ENV, args, model, st_seed, test_num=100):
    TASK_ENV.suc = 0
    TASK_ENV.test_num = 0
    succ_seed = 0
    succ_test_seed_list = []

    while succ_seed < test_num:
        # A. 专家验证阶段
        if expert_check:
            TASK_ENV.setup_demo(seed=now_seed, is_test=True, **args)
            episode_info = TASK_ENV.play_once()  # 执行专家轨迹
            TASK_ENV.close_env()

            # 验证专家轨迹是否成功
            if TASK_ENV.plan_success and TASK_ENV.check_success():
                succ_seed += 1
                suc_test_seed_list.append(now_seed)

        # B. 策略评测阶段
        TASK_ENV.setup_demo(seed=now_seed, is_test=True, **args)

        # 生成语言指令
        instruction = np.random.choice(
            generate_episode_descriptions(args["task_name"], episode_info_list, test_num)[0][instruction_type]
        )
        TASK_ENV.set_instruction(instruction)

        # C. 策略执行
        while TASK_ENV.take_action_cnt < TASK_ENV.step_lim:
            observation = TASK_ENV.get_obs()
            eval_func(TASK_ENV, model, observation)  # 调用策略推理

            if TASK_ENV.eval_success:
                break

        # D. 记录结果
        if succ:
            TASK_ENV.suc += 1

        TASK_ENV.test_num += 1
        now_seed += 1

    return now_seed, TASK_ENV.suc
```

#### 客户端-服务器评测 (`eval_policy_client.py`)

支持远程评测，通过Socket通信：

```python
class ModelClient:
    def __init__(self, host='localhost', port=9999, timeout=30):
        self._connect()  # 自动重连，最多1000次尝试

    def call(self, func_name=None, obs=None):
        # 序列化观测（支持numpy数组）
        json_data = numpy_to_json({"cmd": func_name, "obs": obs})

        # 发送长度和数据
        self.sock.sendall(len(json_data).to_bytes(4, 'big'))
        self.sock.sendall(json_data.encode('utf-8'))

        # 接收响应
        response = self._recv_response()
        return response['res']
```

使用方式：
```bash
# 服务端
python script/policy_model_server.py --port 9999

# 客户端
python script/eval_policy_client.py \
    --config my_config.yml \
    --port 9999
```

### 3.3 评测指标

#### 主要指标

1. **ASR (Average Success Rate)**：平均成功率
   - 公式：`suc_num / test_num`
   - 值域：0-100%

2. **Top5-ASR**：Top-5候选成功率
   - 用于代码生成阶段，表示在5个候选程序中的最高成功率

3. **CR-Iter**：平均修正迭代次数
   - 在专家代码生成中，达到50%成功率前的平均迭代轮数
   - 越低表示生成效率越高

4. **Token**：生成代码的token数
   - 用于衡量LLM推理成本

#### 评测输出格式

```
eval_result/{task_name}/{policy_name}/{task_config}/{ckpt_setting}/{timestamp}/
└── _result.txt
```

内容示例：
```
Timestamp: 2025-08-06 15:30:45

Instruction Type: template

0.81
0.75
0.92
...
```

### 3.4 成功判断机制

#### 任务级成功判断

每个任务子类实现 `check_success()` 方法：

```python
# 示例1：beat_block_hammer
def check_success(self):
    hammer_target_pose = self.hammer.get_functional_point(0, "pose").p
    block_pose = self.block.get_functional_point(1, "pose").p
    eps = np.array([0.02, 0.02])
    return np.all(abs(hammer_pose[:2] - block_pose[:2]) < eps) and \
           self.check_actors_contact(self.hammer.get_name(), self.block.get_name())

# 示例2：stack_bowls_two
def check_success(self):
    bowl1_pose = self.bowl1.get_pose()
    bowl2_pose = self.bowl2.get_pose()
    z_dist = abs(bowl1_pose.p[2] - bowl2_pose.p[2])
    xy_dist = np.linalg.norm(bowl1_pose.p[:2] - bowl2_pose.p[:2])
    return z_dist < 0.05 and xy_dist < 0.03 and \
           self.check_actors_contact(self.bowl1, self.bowl2)
```

#### 评测模式成功判断

```python
# 在eval_policy中
if TASK_ENV.eval_success:
    TASK_ENV.suc += 1
    print("\033[92mSuccess!\033[0m")
```

`eval_success`标志在策略执行循环中由以下条件触发：
- 任务特定的 `check_success()` 返回True
- 达到步数限制 `step_lim`

### 3.5 支持的策略类型

评测脚本通过装饰器模式支持多种策略：

```python
def eval_function_decorator(policy_name, model_name):
    policy_model = importlib.import_module(policy_name)
    return getattr(policy_model, model_name)

# 使用示例
get_model = eval_function_decorator("ACT", "get_model")
eval_func = eval_function_decorator("ACT", "eval")
reset_func = eval_function_decorator("ACT", "reset_model")
```

当前支持的策略：
- **ACT** (Action Chunking with Transformer)
- **DP** (Diffusion Policy)
- **DP3** (3D Diffusion Policy)
- **RDT** (Robot Diffusion Transformer 1B)
- **Pi0** (Vision-Language-Action Model)
- **Pi05** (Pi0的5B版本)
- **OpenVLA-oft** (OpenVLA with OpenFineTune)
- **TinyVLA** (小型VLA)
- **DexVLA** (灵巧操作VLA)
- **LLaVA-VLA** (LLaVA-based VLA)

### 3.6 视频录制

支持评测视频录制：

```python
if args["eval_video_log"]:
    video_save_dir = save_dir
    camera_config = get_camera_config(args["camera"]["head_camera_type"])
    video_size = f"{camera_config['w']}x{camera_config['h']}"

    # 启动ffmpeg子进程
    ffmpeg = subprocess.Popen([
        "ffmpeg", "-y", "-loglevel", "error",
        "-f", "rawvideo", "-pixel_format", "rgb24",
        "-video_size", video_size, "-framerate", "10", "-i", "-",
        "-pix_fmt", "yuv420p", "-vcodec", "libx264",
        "-crf", "23",
        f"{TASK_ENV.eval_video_path}/episode{TASK_ENV.test_num}.mp4"
    ], stdin=subprocess.PIPE)

    TASK_ENV._set_eval_video_ffmpeg(ffmpeg)
```

每帧通过管道传输到ffmpeg，生成MP4文件。

---

## 四、域随机化设计

### 4.1 随机化维度

| 维度 | 配置参数 | 影响范围 |
|------|-----------|---------|
| **场景杂乱** | `cluttered_table` | 添加10个干扰物体（从731个物体中采样） |
| **背景纹理** | `random_background`, `clean_background_rate` | 11,000+纹理库（Stable Diffusion生成） |
| **光照条件** | `random_light`, `crazy_random_light_rate` | 颜色、强度、位置、类型随机化 |
| **桌面高度** | `random_table_height` | ±3cm高度变化 |
| **语言指令** | `language_num` | 每个任务100+种指令变体 |

### 4.2 随机化实现

```python
# 在Base_Task.__init__中
self.random_background = random_setting.get("random_background", False)
self.cluttered_table = random_setting.get("cluttered_table", False)
self.clean_background_rate = random_setting.get("clean_background_rate", 1)
self.random_light = random_setting.get("random_light", False)
self.crazy_random_light = np.random.rand() < crazy_random_light_rate
self.random_table_height = random_setting.get("random_table_height", 0)

# 背景纹理随机化
if self.random_background:
    texture_type = "seen" if not self.eval_mode else "unseen"
    directory_path = f"./assets/background_texture/{texture_type}"
    wall_texture, table_texture = np.random.randint(0, file_count), np.random.randint(0, file_count)

    if np.random.rand() <= self.clean_background_rate:
        self.wall_texture = None

# 杂乱物体生成
if self.cluttered_table:
    self.get_cluttered_table(cluttered_numbers=10,
                                  xlim=[-0.59, 0.59],
                                  ylim=[-0.34, 0.34])
```

---

## 五、机器人平台支持

### 5.1 支持的机器人

| 机器人 | DoF | 特点 | 适用任务类型 |
|--------|-----|------|------------|
| **Aloha-AgileX** | 6 DoF × 2 | 低成本、桌面操作 |
| **Franka** | 7 DoF | 高精度、工业级 |
| **UR5** | 7 DoF | 工业级、快速 |
| **Piper** | 6 DoF | 紧凑型、受限空间 |
| **ARX-X5** | 6 DoF | 移动平台 |

### 5.2 跨机器人配置

```python
# 单机器人双臂模式
embodiment: [aloha-agilex]
# 同一机器人的左右臂
args["dual_arm_embodied"] = True
args["left_robot_file"] = get_embodiment_file(embodiment_type[0])
args["right_robot_file"] = get_embodiment_file(embodiment_type[0])

# 双机器人模式
embodiment: [aloha-agilex, franka, 0.5]
args["dual_arm_embodied"] = False  # 两个独立机器人
args["left_robot_file"] = get_embodiment_file(embodiment_type[0])
args["right_robot_file"] = get_embodiment_file(embodiment_type[1])
args["embodiment_dis"] = embodiment_type[2]  # 两机器人距离
```

### 5.3 身体感知抓取适应

```python
# 每个物体标注多个抓取候选
# 考虑机器人特定的可达空间和抓取偏好

def choose_grasp_pose(self, actor, arm_tag, pre_dis=0.1):
    # 评估不同抓取方向的可行性
    for contact_point_id, _ in actor.iter_contact_points():
        pre_pose = self.get_grasp_pose(actor, arm_tag,
                                        contact_point_id=i,
                                        pre_dis=pre_dis)

        # 检查轨迹规划是否成功
        if plan_func(pre_pose)["status"] == "Success":
            return pre_pose, pose

    # 为低DoF机器人返回更多候选
    return self._default_choose_grasp_pose(actor, arm_tag, pre_dis)
```

---

## 六、评测特色

### 6.1 专家验证机制

评测采用两阶段验证：

**阶段1：专家轨迹验证**
```python
TASK_ENV.setup_demo(seed=now_seed, is_test=True, **args)
episode_info = TASK_ENV.play_once()

if TASK_ENV.plan_success and TASK_ENV.check_success():
    succ_seed += 1
    suc_test_seed_list.append(now_seed)
else:
    now_seed += 1  # 跳过此种子，继续尝试
```

**阶段2：策略执行**
```python
TASK_ENV.setup_demo(seed=now_seed, is_test=True, **args)
instruction = generate_random_instruction(TASK_ENV.info["info"])

while TASK_ENV.take_action_cnt < TASK_ENV.step_lim:
    obs = TASK_ENV.get_obs()
    eval_func(TASK_ENV, model, obs)
    if TASK_ENV.eval_success:
        break
```

**优点：**
- 确保评测种子对应可解的场景
- 过滤掉不稳定的物理状态
- 提供语言指令条件下的公平评估

### 6.2 双模式评测

**Easy模式** (`demo_clean.yml`)：
- 无域随机化
- 测试模型在理想环境下的性能
- 基准性能参考

**Hard模式** (`demo_randomized.yml`)：
- 完整域随机化
- 测试模型鲁棒性
- 接近真实场景的挑战

### 6.3 语言条件评测

```python
# 指令类型多样化
instruction_type = "template"  # 模板指令
instruction_type = "semantic"  # 语义多样化
instruction_type = "descriptive"  # 描述性

# 从预生成池中采样
instruction = np.random.choice(
    generate_episode_descriptions(args["task_name"],
                          episode_info_list,
                          test_num)[0][instruction_type]
)
TASK_ENV.set_instruction(instruction)
```

每个任务对象有：
- 60种指令模板（训练用50，评测用10）
- 15种物体描述（训练用12，评测用3）
- 组合生成多样化的指令

### 6.4 稳定性检查

在任务初始化时进行：

```python
def check_stable(self):
    actors_list = [actor for actor in self.scene.get_all_actors()]
    actors_pose_list = []

    # 运行2000步让物体稳定
    for _ in range(2000):
        self.scene.step()

    for actor in actors_list:
        actors_pose_list.append([actor.get_pose()])

    # 再运行500步检查稳定性
    for _ in range(500):
        self.scene.step()

    # 检查最后200步的姿态变化
    for idx, actor in enumerate(actors_list):
        final_pose = actors_pose_list[idx][-1]
        for pose in actors_pose_list[idx][-200:]:
            if cal_quat_dis(final_pose.q, pose.q) * 180 > 3.0:
                is_stable = False
                unstable_list.append(actor.get_name())

    if not is_stable:
        raise UnStableError(f'Objects unstable: {unstable_list}')
```

不稳定的场景会被跳过，寻找稳定种子。

---

## 七、评测输出与结果分析

### 7.1 输出文件结构

```
eval_result/
└── {task_name}/
    └── {policy_name}/
        └── {task_config}/
            └── {ckpt_setting}/
                ├── {timestamp}/
                │   ├── _result.txt      # 成功率
                │   └── video/           # 评测视频
                └── ...
```

### 7.2 排行榜格式

[RoboTwin Leaderboard](https://robotwin-platform.github.io/leaderboard) 标准格式：

| 模型 | Easy | Hard | 平均 |
|------|------|------|------|
| RDT | 28.0% | 1.7% | 14.7% |
| Pi0 | 46.4% | 16.3% | 29.1% |
| ACT | 2.0% | 0.6% | 1.7% |
| DP | 0.0% | 0.0% | 0.0% |
| DP3 | 55.2% | 5.0% | 28.0% |

### 7.3 评测报告信息

```python
file_path = os.path.join(save_dir, f"_result.txt")
with open(file_path, "w") as file:
    file.write(f"Timestamp: {current_time}\n\n")
    file.write(f"Instruction Type: {instruction_type}\n\n")
    file.write("\n".join(map(str, np.array(suc_nums) / test_num)))
```

实时打印：
```python
print(
    f"\033[93m{task_name}\033[0m | "
    f"\033[94m{args['policy_name']}\033[0m | "
    f"\033[92m{args['task_config']}\033[0m | "
    f"\033[91m{args['ckpt_setting']}\033[0m\n"
    f"Success rate: \033[96m{TASK_ENV.suc}/{TASK_ENV.test_num}\033[0m => "
    f"\033[95m{round(TASK_ENV.suc/TASK_ENV.test_num*100, 1)}%\033[0m, "
    f"current seed: \033[90m{now_seed}\033[0m\n"
)
```

---

## 八、评测系统的优势与设计亮点

### 8.1 设计优势

1. **模块化架构**：策略、环境、评测完全解耦
2. **统一接口**：通过装饰器模式支持任意策略
3. **灵活配置**：YAML配置支持Easy/Hard切换
4. **专家验证**：确保评测场景的可解性
5. **稳定检查**：自动过滤不稳定的物理状态
6. **多语言支持**：测试指令鲁棒性
7. **跨平台支持**：5种机器人，异构配置
8. **远程评测**：客户端-服务器模式
9. **视频录制**：完整的可复现评测结果
10. **实时反馈**：彩色终端输出，实时进度

### 8.2 评测流程亮点

- **两阶段验证**：专家验证 → 策略执行，确保场景质量
- **自适应种子搜索**：自动跳过不稳定场景，保证100个可解种子
- **步数限制**：每个任务有独立步数上限（400-1700）
- **缓存清理**：定期清理渲染缓存，避免内存泄漏
- **并行支持**：支持双机器人协作评测

---

## 总结

RoboTwin的评测系统是一个**设计严谨、功能完备**的框架：

1. **标准化**：统一的评测协议，支持不同策略公平比较
2. **鲁棒性测试**：Hard模式全面测试环境变化适应性
3. **可扩展性**：易于添加新任务、新策略、新机器人
4. **质量保证**：专家验证 + 稳定性检查，确保评测有效性
5. **实用性**：远程评测、视频录制、实时反馈，方便调试

这使得RoboTwin成为**双臂机器人操作**领域的重要基准平台，为政策研究和sim-to-real转移提供了可靠的评测基础设施。
