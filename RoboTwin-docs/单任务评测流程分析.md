# RoboTwin 单任务评测流程详细分析

## 概述

本文档以 `stack_bowls_two`（堆叠两个碗）任务为例，详细说明RoboTwin的单任务完整评测流程。

---

## 一、评测流程总览

```
┌─────────────────────────────────────────────────────────────────────────┐
│  1. 配置加载与初始化                                    │
│  2. 专家轨迹生成（数据收集）                             │
│  3. 稳定性检查                                        │
│  4. 策略评测（100次测试）                             │
│  5. 结果记录与输出                                        │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 二、详细流程分析

### 阶段1：配置加载与初始化

#### 1.1 配置文件解析

**配置来源**：`task_config/demo_randomized.yml`

```yaml
render_freq: 0                    # 不渲染，加速评测
episode_num: 50                    # 数据收集用50条轨迹
use_seed: false                     # 随机种子
save_freq: 15                       # 每15步保存一次数据
embodiment: [aloha-agilex]          # 单机器人双臂
language_num: 100                     # 指令多样性

domain_randomization:
  random_background: true             # 随机背景纹理
  cluttered_table: true             # 添加10个干扰物体
  clean_background_rate: 0.02      # 2%概率清洁背景
  random_head_camera_dis: 0         # 相机距离固定
  random_table_height: 0.03          # 桌高±3cm
  random_light: true                 # 随机光照
  crazy_random_light_rate: 0.02      # 2%极端光照

camera:
  head_camera_type: D435            # 头部相机
  wrist_camera_type: D435           # 腕部相机
  collect_head_camera: true           # 收集头相机数据
  collect_wrist_camera: true          # 收集腕相机数据

data_type:
  rgb: true                        # RGB图像
  depth: false                     # 不收集深度
  pointcloud: false                 # 不收集点云
  endpose: true                    # 端部位姿
  qpos: true                       # 关节位置
  actor_segmentation: false         # 不收集分割

save_path: ./data                  # 数据保存路径
clear_cache_freq: 5                # 每5轮清理缓存
eval_video_log: true               # 录制评测视频
```

**关键特性解读**：
- **域随机化维度**：5个维度（杂乱、背景、光照、高度、语言）
- **数据类型**：支持RGB、深度、点云、端姿、关节位置等
- **机器人配置**：Aloha-AgileX（6 DoF双臂）
- **评测步数限制**：从`_eval_step_limit.yml`读取，stack_bowls_two为900步

#### 1.2 环境初始化流程

```python
# script/eval_policy.py main() 函数中：

# 1. 创建任务环境实例
TASK_ENV = class_decorator("stack_bowls_two")  # 动态导入envs/stack_bowls_two.py

# 2. 设置机器人配置
args["left_robot_file"] = get_embodiment_file("aloha-agilex")
args["right_robot_file"] = get_embodiment_file("aloha-agilex")
args["dual_arm_embodied"] = True  # 单机器人双臂

# 3. 加载机器人配置
args["left_embodiment_config"] = get_embodiment_config(args["left_robot_file"])
args["right_embodiment_config"] = get_embodiment_config(args["right_robot_file"])

# 4. 加载相机配置
with open(CONFIGS_PATH + "_camera_config.yml") as f:
    _camera_config = yaml.load(f.read())
args["head_camera_h"] = _camera_config["D435"]["h"]  # 480
args["head_camera_w"] = _camera_config["D435"]["w"]  # 640
```

**Base_Task初始化**（在`envs/_base_task.py`中）：

```python
def _init_task_env_(self, **kwargs):
    # 1. 设置随机种子
    np.random.seed(kwargs.get("seed", 0))
    torch.manual_seed(kwargs.get("seed", 0))

    # 2. 初始化渲染引擎
    self.engine = sapien.Engine()
    self.renderer = sapien.SapienRenderer()
    self.scene = self.engine.create_scene(scene_config)

    # 3. 配置光照和材质
    sapien.render.set_camera_shader_dir("rt")
    sapien.render.set_ray_tracing_samples_per_pixel(32)

    # 4. 添加光源（支持随机化）
    if self.random_light:
        # 随机化光源颜色、位置、强度
        for light in self.direction_light_lst + self.point_light_lst:
            light.set_color([np.random.rand(), np.random.rand(), np.random.rand()])

    # 5. 设置物理参数
    self.scene.set_timestep(1/250)  # 4ms物理步长
    self.scene.default_physical_material = self.scene.create_physical_material(0.5, 0.5, 0)

    # 6. 配置域随机化参数
    self.random_background = random_setting.get("random_background", False)
    self.cluttered_table = random_setting.get("cluttered_table", False)
    self.clean_background_rate = random_setting.get("clean_background_rate", 1)
    self.random_table_height = random_setting.get("random_table_height", 0)

    # 7. 初始化评测相关属性
    self.eval_mode = kwargs.get("eval_mode", False)
    self.eval_success = False
    self.take_action_cnt = 0
    self.step_lim = None  # 从_eval_step_limit.yml读取
    self.instruction = None
```

---

### 阶段2：专家轨迹生成（数据收集）

**目的**：生成专家演示轨迹，用于：
1. 验证场景可解性
2. 提供训练数据
3. 确保评测种子对应稳定场景

#### 2.1 场景物体加载流程

**stack_bowls_two任务**（`envs/stack_bowls_two.py`）：

```python
class stack_bowls_two(Base_Task):
    def setup_demo(self, **kwargs):
        super()._init_task_env_(**kwargs)

        # 1. 加载碗模型
        bowl_pose_lst = []
        for i in range(2):
            # 随机生成碗的位置（x: -0.3~0.3, y: -0.15~0.15）
            bowl_pose = rand_pose(
                xlim=[-0.3, 0.3],
                ylim=[-0.15, 0.15],
                zlim=[0.74],  # z=table height
                qpos=[0.5, 0.5, 0.5, 0.5],  # 初始朝向朝上
                ylim_prop=True,
                rotate_rand=False,  # 不随机旋转
            )

            # 检查碗之间距离（避免重叠）
            def check_bowl_pose(bowl_pose):
                # 碗碗中心点距离 < 0.16 或 xy距离 < 0.0169
                for j in range(len(bowl_pose_lst)):
                    if (np.sum(pow(bowl_pose.p[:2] - bowl_pose_lst[j].p[:2], 2)) < 0.0169
                        return False
                return True

            # 如果重叠，重新生成位置
            while not check_bowl_pose(bowl_pose):
                bowl_pose = rand_pose(...)  # 重新随机

            bowl_pose_lst.append(bowl_pose)

        # 2. 创建碗的Actor对象
        self.bowl1 = create_actor(
            scene=self,
            pose=bowl_pose_lst[0],
            modelname="002_bowl",  # 从RoboTwin-OD加载
            model_id=3,  # 碗的第3个模型
            convex=True,  # 凸包优化碰撞检测
        )
        self.bowl2 = create_actor(bowl_pose_lst[1], ...)

        # 3. 设置禁止区域（碗的位置不应被其他物体干扰）
        self.add_prohibit_area(self.bowl1, padding=0.07)
        self.add_prohibit_area(self.bowl2, padding=0.07)

        # 4. 设置堆叠目标位置
        target_pose = [-0.1, -0.15, 0.1, -0.05, 0.5, 0.5, 0.5]
        self.prohibited_area.append(target_pose)

        # 5. 记录目标位置和四元数
        self.bowl1_target_pose = np.array([-0.1, -0.15, 0.76])  # 加上table_z_bias
        self.quat_of_target_pose = [0, 0.707, 0.707, 0]  # 目标旋转四元数
```

#### 2.2 杂乱场景生成（如果启用）

```python
# 在Base_Task.setup_scene()中

def get_cluttered_table(self, cluttered_numbers=10,
                           xlim=[-0.59, 0.59],
                           ylim=[-0.34, 0.34],
                           zlim=[0.741]):

    # 1. 获取可用干扰物体
    task_objects_list = []
    for entity in self.scene.get_all_actors():
        actor_name = entity.get_name()
        if actor_name in ["table", "wall", "ground"]:
            continue  # 排除场景基础物体
        task_objects_list.append(actor_name)

    # 2. 从RoboTwin-OD加载物体元数据
    self.obj_names, self.cluttered_item_info = get_available_cluttered_objects(task_objects_list)

    # 3. 随机选择10个干扰物体
    success_count = 0
    max_try = 50
    while success_count < cluttered_numbers and trys < max_try:
        obj = np.random.randint(len(self.obj_names))
        obj_name = self.obj_names[obj]
        obj_idx = np.random.randint(len(self.cluttered_item_info[obj_name]["ids"]))
        obj_radius = self.cluttered_item_info[obj_name]["params"][obj_idx]["radius"]

        # 4. 尝试放置（考虑禁止区域）
        success, cluttered_obj = rand_create_cluttered_actor(
            self.scene,
            xlim=xlim,
            ylim=ylim,
            zlim=np.array(zlim) + self.table_z_bias,
            modelname=obj_name,
            modelid=obj_idx,
            modeltype=self.cluttered_item_info[obj_name]["type"],
            rotate_rand=True,
            rotate_lim=[0, 0, 0.5*math.pi],  # 随机z轴旋转
            size_dict=self.size_dict,  # 记录已放置物体用于碰撞检测
            obj_radius=obj_radius,
            z_offset=obj_z_offset,
            z_max=obj_z_max,
            prohibited_area=self.prohibited_area,  # 避开碗的位置
        )

        if success:
            self.cluttered_obj = cluttered_obj
            self.size_dict.append([
                cluttered_obj.get_pose().p.tolist(),
                obj_radius
            ])
            success_count += 1

    # 5. 记录干扰物体信息
    self.record_cluttered_objects = [
        {"object_type": obj_name, "object_index": obj_idx}
        for each cluttered object
    ]
```

#### 2.3 背景纹理随机化（如果启用）

```python
# 在Base_Task.create_table_and_wall()中

if self.random_background:
    texture_type = "seen" if not self.eval_mode else "unseen"
    directory_path = f"./assets/background_texture/{texture_type}"

    # 11,000+纹理库，通过Stable Diffusion生成
    file_count = len([name for name in os.listdir(directory_path)
                       if os.path.isfile(os.path.join(directory_path, name))])

    # 随机选择墙和桌子纹理
    wall_texture, table_texture = np.random.randint(0, file_count), np.random.randint(0, file_count)

    # clean_background_rate控制：2%概率使用清洁背景（None表示清洁）
    if np.random.rand() <= self.clean_background_rate:
        self.wall_texture = None
        self.table_texture = None

# 使用纹理创建场景
self.wall = create_box(
    self.scene,
    pose=sapien.Pose(p=[0, 1, 1.5]),  # 墙面位置
    half_size=[3, 0.6, 1.5],
    color=(1, 0.9, 0.9),
    name="wall",
    texture_id=self.wall_texture,
    is_static=True,
)

self.table = create_table(
    self.scene,
    pose=sapien.Pose(p=[table_xy_bias[0], table_xy_bias[1], table_height]),
    length=1.2,  # 桌长1.2m
    width=0.7,   # 桌宽0.7m
    height=table_height,
    thickness=0.05,
    is_static=True,
    texture_id=self.table_texture,
)
```

#### 2.4 专家轨迹生成（play_once）

```python
class stack_bowls_two(Base_Task):
    def play_once(self):
        # 初始化使用的机械臂
        self.las_arm = None

        # 步骤1：移动bowl1到堆叠位置
        # 选择左右臂（根据bowl1的x坐标）
        arm_tag1 = ArmTag("left" if bowl1_pose[0] < 0 else "right")

        # 抓取bowl1（使用抓取点0，即碗的中心底部）
        arm_tag1 = self.move_bowl(
            self.bowl1,
            self.bowl1_target_pose,  # 目标：在bowl2上方堆叠
            arm_tag=arm_tag1
        )

        # 步骤2：移动bowl2到bowl1上方
        # 选择左右臂（同上）
        arm_tag2 = arm_tag1  # 可能用同一只手臂

        # 抓取bowl2（使用抓取点0）
        arm_tag2 = self.move_bowl(
            self.bowl2,
            self.bowl1.get_pose().p + [0, 0, 0.05],  # 在bowl1上方5cm
            arm_tag=arm_tag2
        )

        # 步骤3：放置bowl2
        # 沿z轴上升9cm以避免堆叠时碰撞
        self.move(self.move_by_displacement(arm_tag=arm_tag2, z=0.09, move_axis="arm"))

        # 步骤4：放置bowl2到目标位置
        self.move(
            self.place_actor(
                actor=self.bowl2,
                target_pose=self.bowl1_target_pose.tolist() + self.quat_of_target_pose,
                arm_tag=arm_tag2,
                functional_point_id=0,  # 使用碗的中心功能点
                pre_dis=0.09,
                dis=0,
                constrain="align",  # 对齐到目标位置
                pre_dis_axis="fp",  # 从功能点方向对齐
            )
        )

        # 步骤5：双手打开夹爪
        self.together_open_gripper(save_freq=None)

        # 记录episode信息
        self.info["info"] = {
            "{A}": "002_bowl/base3",     # 物体类型
            "{B}": "002_bowl/base3",
            "{a}": str(arm_tag1),  # 哪个臂抓了bowl1
            "{b}": str(arm_tag2),  # 哪个臂抓了bowl2
        }

        return self.info
```

---

### 阶段3：稳定性检查

#### 3.1 检查目的

确保场景物理稳定，物体不会因物理引擎初始化问题而移动或倒塌。

#### 3.2 检查实现（在Base_Task中）

```python
def check_stable(self):
    actors_list, actors_pose_list = [], []

    # 1. 获取所有Actor
    for actor in self.scene.get_all_actors():
        actors_list.append(actor)

    # 2. 运行2000步物理模拟让物体稳定
    for _ in range(2000):
        self.scene.step()

    # 3. 记录初始姿态
    for actor in actors_list:
        actors_pose_list.append([actor.get_pose()])

    # 4. 再运行500步检查长期稳定性
    for _ in range(500):
        self.scene.step()

    # 5. 检查最后200步的姿态变化
    for idx, actor in enumerate(actors_list):
        final_pose = actors_pose_list[idx][-1]
        for pose in actors_pose_list[idx][-200:]:
            # 计算姿态变化（四元数距离转换为角度）
            if cal_quat_dis(final_pose.q, pose.q) * 180 > 3.0:  # 超过3度变化
                is_stable = False
                unstable_list.append(actor.get_name())

    if not is_stable:
        raise UnStableError(f'Objects unstable: {unstable_list}')

    return is_stable, unstable_list
```

**检查时机**：
- 在`setup_demo`结束时执行
- 如果检查失败，抛出`UnStableError`异常

#### 3.3 异常处理

```python
# 在eval_policy.py的expert_check阶段

try:
    TASK_ENV.setup_demo(now_ep_num=now_id, seed=now_seed, is_test=True, **args)
    episode_info = TASK_ENV.play_once()
    TASK_ENV.close_env()
except UnStableError as e:
    print(" -------------")
    print("Error: ", e)
    print(" -------------")
    TASK_ENV.close_env()
    now_seed += 1  # 跳过此种子，继续尝试
    continue
```

---

### 阶段4：语言指令生成

#### 4.1 指令来源

**任务指令模板**：`task_instruction/stack_bowls_two.json`

```json
{
  "seen": [
    "Stack {A} on top of {B}",
    "Stack {A} above {B}",
    "Place {A} on {B}",
    "Use {a} arm to stack {A} on {B}",
    ...
  ],
  "unseen": [
    "Stack the {A} on the {B}",
    "Place the {A} above the {B}",
    ...
  ]
}
```

#### 4.2 物体描述数据（RoboTwin-OD）

每个物体有15个描述，示例`002_bowl.json`：

```json
{
  "seen": [
    "A medium-sized yellow bowl",
    "A light blue ceramic bowl",
    "A small wooden bowl",
    ...
  ],
  "unseen": [
    "A green metal bowl",
    "A dark plastic bowl",
    ...
  ]
}
```

#### 4.3 指令生成流程（在`eval_policy.py`中）

```python
# 在评测循环的专家验证成功后

# 1. 调用指令生成函数
episode_info_list = [episode_info["info"]]
results = generate_episode_descriptions(
    args["task_name"],  # "stack_bowls_two"
    episode_info_list,
    test_num  # 100
)

# 2. results[0][instruction_type]包含100条生成的指令
# instruction_type可以是：
#   - "template": 使用模板指令
#   - "semantic": 使用语义多样化
#   "descriptive": 使用描述性指令

# 3. 随机选择一条指令
instruction = np.random.choice(results[0][instruction_type])

# 4. 设置给环境
TASK_ENV.set_instruction(instruction)
```

**生成的指令示例**：
- "Stack 002_bowl/base3 on top of 002_bowl/base3"  # 模板
- "Use left arm to stack the medium-sized yellow bowl on the light blue ceramic bowl"  # 语义+描述
- "Place the green metal bowl on the light blue ceramic bowl"  # 描述性

---

### 阶段5：策略评测

#### 5.1 评测循环结构

```python
# 在eval_policy()函数中

st_seed = 100000 * (1 + seed)  # 初始种子（避免与数据收集冲突）
suc_nums = []
test_num = 100  # 评测100个episode
succ_seed = 0
succ_test_seed_list = []
TASK_ENV.suc = 0
TASK_ENV.test_num = 0

while succ_seed < test_num:
    # 循环直到找到100个成功的场景
```

#### 5.2 专家验证阶段（每个测试episode）

```python
while succ_seed < test_num:
    render_freq = args["render_freq"]  # 保存原设置
    args["render_freq"] = 0  # 关闭渲染，加速

    # === 子阶段A：专家轨迹验证 ===
    if expert_check:
        try:
            # 1. 使用测试种子重置环境
            TASK_ENV.setup_demo(
                now_ep_num=now_id,
                seed=now_seed,
                is_test=True,  # 标记为评测模式
                **args
            )

            # 2. 执行专家轨迹（play_once）
            episode_info = TASK_ENV.play_once()

            # 3. 关闭环境
            TASK_ENV.close_env()

        except UnStableError as e:
            # 场景不稳定，跳过此种子
            now_seed += 1
            continue

        # 4. 检查专家轨迹是否成功完成
        if TASK_ENV.plan_success and TASK_ENV.check_success():
            succ_seed += 1
            suc_test_seed_list.append(now_seed)
        else:
            # 专家轨迹失败，跳过
            now_seed += 1
            continue

    # 恢复渲染设置
    args["render_freq"] = render_freq
```

**plan_success标志**：
- 在`play_once`的每个`self.move()`调用中设置
- 如果所有动作规划成功，则`plan_success=True`
- 如果任何动作规划失败（超出工作空间），则`plan_success=False`

#### 5.3 策略执行阶段（每个成功的测试episode）

```python
# === 子阶段B：策略执行 ===

# 1. 重置环境（使用相同的种子）
TASK_ENV.setup_demo(
    now_ep_num=now_id,
    seed=now_seed,  # 使用验证通过的种子
    is_test=True,
    **args
)

# 2. 生成语言指令
episode_info_list = [episode_info["info"]]
instruction = np.random.choice(
    generate_episode_descriptions(args["task_name"], episode_info_list, test_num)[0][instruction_type]
)
TASK_ENV.set_instruction(instruction)

# 3. 启动视频录制（如果启用）
if TASK_ENV.eval_video_path is not None:
    camera_config = get_camera_config(args["camera"]["head_camera_type"])
    video_size = f"{camera_config['w']}x{camera_config['h']}"
    
    ffmpeg = subprocess.Popen([
        "ffmpeg", "-y", "-loglevel", "error",
        "-f", "rawvideo", "-pixel_format", "rgb24",
        "-video_size", video_size, "-framerate", "10",
        "-i", "-",  # 从stdin读取
        "-pix_fmt", "yuv420p", "-vcodec", "libx264",
        "-crf", "23",
        f"{TASK_ENV.eval_video_path}/episode{TASK_ENV.test_num}.mp4"
    ], stdin=subprocess.PIPE)
    
    TASK_ENV._set_eval_video_ffmpeg(ffmpeg)

# 4. 重置策略模型
reset_func(model)  # e.g., reset_state()

# 5. 策略执行循环
TASK_ENV.take_action_cnt = 0
succ = False

while TASK_ENV.take_action_cnt < TASK_ENV.step_lim:  # step_lim=900
    # 5.1 获取观测
    observation = TASK_ENV.get_obs()
    
    # observation包含：
    # - observation["head_camera"]["rgb"]: 640x480x3 RGB图像
    # - observation["wrist_left_camera"]["rgb"]: 640x480x3 RGB图像
    # - observation["wrist_right_camera"]["rgb"]: 640x480x3 RGB图像
    # - endpose["left_endpose"]: 左臂7D位姿
    # - endpose["right_endpose"]: 右臂7D位姿
    # - endpose["left_gripper"]: 左夹爪开合度（归一化）
    # - endpose["right_gripper"]: 右夹爪开合度
    # - joint_action["vector"]: 14维关节位置（6+6+6+1+1）
    
    # 5.2 策略推理
    eval_func(TASK_ENV, model, observation)
    # 根据策略类型不同：
    #   ACT: 调用策略模型预测动作序列
    #   DP3: 使用3D点云预测
    #   RDT/Pi0: 视觉-语言-动作模型推理
    
    # 5.3 更新动作计数
    TASK_ENV.take_action_cnt += 1
    
    # 5.4 检查是否成功
    if TASK_ENV.eval_success:
        succ = True
        break  # 成功则提前结束

# 6. 记录结果
if succ:
    TASK_ENV.suc += 1
else:
    print("\033[91mFail!\033[0m")

# 7. 关闭环境
TASK_ENV.close_env(clear_cache=((succ_seed + 1) % args["clear_cache_freq"] == 0))

# 8. 更新测试计数
TASK_ENV.test_num += 1
now_seed += 1

# 9. 实时打印进度
print(
    f"\033[93m{task_name}\033[0m | "
    f"\033[94m{args['policy_name']}\033[0m | "
    f"\033[92m{args['task_config']}\033[0m | "
    f"\033[91m{args['ckpt_setting']}\033[0m\n"
    f"Success rate: \033[96m{TASK_ENV.suc}/{TASK_ENV.test_num}\033[0m => "
    f"\033[95m{round(TASK_ENV.suc/TASK_ENV.test_num*100, 1)}%\033[0m, "
    f"current seed: \033[90m{now_seed}\033[0m\n"
)
```

---

### 阶段6：成功判断机制

#### 6.1 check_success实现

```python
class stack_bowls_two(Base_Task):
    def check_success(self):
        # 获取两个碗的位置
        bowl1_pose = self.bowl1.get_pose().p  # [x, y, z]
        bowl2_pose = self.bowl2.get_pose().p

        # 对碗按x坐标排序
        bowls = sorted([bowl1_pose, bowl2_pose], key=lambda x: x[2])

        # 目标高度（考虑桌子高度偏移）
        target_height = [
            0.74 + self.table_z_bias,  # bowl1的高度
            0.77 + self.table_z_bias,  # bowl2在bowl1上方的高度
        ]

        # 容差：xy方向±2cm，z方向±2cm
        eps = np.array([0.02, 0.02])
        eps2 = np.array([0.02, 0.02])

        # 检查条件：
        # 1. bowl2在bowl1上方（z方向正确）
        # 2. bowl2在目标x和y范围内
        condition1 = np.all(np.array([bow2_pose[2], bowl2_pose[2]]) - target_height < eps)
        
        # 3. 双爪均打开（释放碗）
        condition2 = self.is_left_gripper_open() and self.is_right_gripper_open()

        return condition1 and condition2
```

#### 6.2 评测中的成功判断

```python
# 在Base_Task中，eval_success标志由以下方式触发：

# 方式1：任务特定的check_success()返回True
if TASK_ENV.check_success():
    TASK_ENV.eval_success = True

# 方式2：达到步数限制
if TASK_ENV.take_action_cnt >= TASK_ENV.step_lim:
    # 不触发成功判断，记录为失败
    # 但仍然完成当前episode

# 方式3：在策略执行循环中显式设置
# 某些策略可能会在内部直接设置eval_success标志
```

---

### 阶段7：结果记录与输出

#### 7.1 文件结构

```
eval_result/
└── stack_bowls_two/
    └── demo_randomized/
        └── {policy_name}/
            └── {ckpt_setting}/
                ├── {timestamp}/
                │   ├── _result.txt          # 成功率
                │   ├── video/               # 评测视频
                │   └── ...
                └── ...
```

#### 7.2 _result.txt格式

```
Timestamp: 2025-08-06 15:30:45

Instruction Type: template

0.72
0.68
0.81
...
0.76
```

**数据含义**：
- 每行一个episode的成功/失败结果
- 1 = 成功，0 = 失败
- 共100个数值（对应100次测试）
- 成功率 = sum(suc_nums) / 100

#### 7.3 视频文件

```
video/
├── episode0.mp4
├── episode1.mp4
├── ...
└── episode99.mp4
```

每个视频对应一个评测episode，包含：
- 头部相机视角（640x480, 10fps）
- 原始RGB24 → H.264编码
- 完整任务执行过程

---

## 三、评测流程关键特性总结

### 3.1 两阶段验证设计

```
专家验证（expert_check=True）
    ├── 使用测试种子重置环境
    ├── 执行专家轨迹（play_once）
    ├── 检查plan_success和check_success
    └── 仅在专家轨迹成功时进入策略评测

策略执行（expert_check=False）
    ├── 使用相同的成功种子
    ├── 生成语言指令
    ├── 加载策略模型
    ├── 在step_lim步数内执行
    └── 实时记录成功/失败
```

**优点**：
- 确保评测场景的可解性（专家能完成）
- 过滤物理不稳定的场景
- 提供语言条件下的公平评估

### 3.2 域随机化的五个维度

| 维度 | 实现机制 | 影响范围 |
|------|----------|---------|
| **场景杂乱** | 随机选择10个干扰物体，避免碰撞 | 增加视觉复杂度和推理难度 |
| **背景纹理** | 11,000+纹理库（Stable Diffusion生成） | 测试视觉鲁棒性 |
| **光照条件** | 随机颜色、强度、位置、类型 | 测试光照变化鲁棒性 |
| **桌面高度** | ±3cm随机变化 | 测试不同工作平面下的性能 |
| **语言指令** | 100+种指令变体（模板/语义/描述） | 测试语言理解鲁棒性 |

### 3.3 观测数据结构

```python
observation = {
    "observation": {
        "head_camera": {
            "rgb": np.ndarray,  # (480, 640, 3) RGB图像
            "config": {"w": 640, "h": 480}
        },
        "wrist_left_camera": {
            "rgb": np.ndarray,  # (480, 640, 3) RGB图像
            "config": {"w": 640, "h": 480}
        },
        "wrist_right_camera": {
            "rgb": np.ndarray,  # (480, 640, 3) RGB图像
            "config": {"w": 640, "h": 480}
        }
    },
    "pointcloud": [],
    "joint_action": {
        "left_arm": np.ndarray,  # (6,) 左臂6关节
        "left_gripper": float,  # 归一化夹爪开合度
        "right_arm": np.ndarray,  # (6,) 右臂6关节
        "right_gripper": float,  # 归一化夹爪开合度
        "vector": np.ndarray  # (14,) 完整关节向量
    },
    "endpose": {
        "left_endpose": np.ndarray,  # (7,) 左臂7D位姿
        "right_endpose": np.ndarray  # (7,) 右臂7D位姿
    }
}
```

### 3.4 步数限制与任务复杂度

从`_eval_step_limit.yml`：

```yaml
stack_bowls_three: 1200  # 堆叠三个碗
stack_bowls_two: 900   # 堆叠两个碗
place_empty_cup: 500
adjust_bottle: 400
...
```

**复杂度与步数关系**：
- 简单任务：400-500步
- 中等任务：900步
- 复杂任务：1200步
- 需要精确堆叠的任务（stack_bowls_two）为900步

---

## 四、评测流程优势与设计亮点

### 4.1 质量保证机制

1. **稳定性检查**：确保评测场景物理稳定
2. **专家验证**：只对专家能完成的场景进行评测
3. **自适应种子搜索**：自动跳过不稳定场景，保证100个可解种子
4. **缓存管理**：定期清理渲染缓存，避免内存泄漏

### 4.2 灵活性与扩展性

1. **模块化设计**：策略、环境、评测完全解耦
2. **统一接口**：通过装饰器支持任意策略类型
3. **多语言支持**：测试指令、语义、描述性三种类型
4. **跨平台支持**：5种机器人，单/双臂模式
5. **远程评测**：客户端-服务器模式

### 4.3 实用性特性

1. **实时反馈**：彩色终端输出，实时显示成功率和进度
2. **视频录制**：完整可复现评测结果
3. **结果标准化**：统一的文件结构和格式
4. **配置驱动**：YAML配置支持Easy/Hard快速切换

---

## 五、总结

RoboTwin的单任务评测流程设计**严谨、功能完备**，主要特点：

1. **两阶段验证**：专家轨迹验证 + 策略执行
2. **五维度域随机化**：全面测试模型鲁棒性
3. **稳定检查机制**：确保评测有效性
4. **模块化架构**：易于扩展和定制
5. **完整的可复现性**：视频录制 + 详细日志

这使得RoboTwin能够为**双臂机器人操作**研究提供标准化、可靠的评测基础设施，支持策略比较、sim-to-real研究以及鲁棒性分析。
