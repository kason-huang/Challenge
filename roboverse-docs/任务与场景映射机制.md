# RoboVerse 任务与场景映射机制

## 概述

RoboVerse与RoboTwin使用**完全不同的映射机制**。RoboVerse采用**基于元仿真（MetaSim）**的统一框架，通过配置类和场景配置实现任务与场景的关联。

**核心差异对比**：

| 方面 | RoboTwin | RoboVerse |
|------|-----------|-----------|
| **架构** | 任务直接继承Base_Task | 基于MetaSim框架，使用ScenarioCfg |
| **场景定义** | task_instruction/JSON指令 | scenes/Python配置类 |
| **映射方式** | scene_info.json存储episode参数 | YAML配置文件直接引用场景类 |
| **数据流** | info字典 → scene_info → 指令生成 | config.yaml → scenario → task环境 |

---

## 一、整体架构

### 1.1 目录结构

```
RoboVerse-main/
├── roboverse_pack/           # 核心包
│   ├── scenes/                # 场景配置（场景类）
│   │   ├── base_scene_cfg.py
│   │   ├── kujiale_scene_0003_cfg.py
│   │   ├── kujiale_scene_0004_cfg.py
│   │   ├── kujiale_scene_0008_cfg.py
│   │   ├── libero_kitchen_tabletop_cfg.py
│   │   ├── manycore_scene_827313_cfg.py
│   │   └── tapwater_scene_131_cfg.py
│   ├── tasks/                 # 任务定义
│   │   ├── pick_place/      # Pick & Place任务
│   │   ├── humanoid/       # 人形机器人任务
│   │   ├── mujoco_playground/  # MuJoCo场景
│   │   ├── rlbench/         # RLBench任务
│   │   └── ...
│   ├── robots/               # 机器人配置
│   ├── grounds/               # 地面配置
│   └── utils/                # 工具函数
└── roboverse_learn/          # 训练和评测脚本
    └── rl/fast_td3/
        └── configs/            # 任务配置文件
```

### 1.2 核心组件

#### 场景配置基类 (SceneCfg)

```python
# roboverse_pack/scenes/base_scene_cfg.py

@configclass
class SceneCfg:
    """Base config class for scenes."""

    # 场景名称
    name: str | None = None

    # 文件路径（支持多种仿真器）
    usd_path: str | None = None      # Isaac Sim (USD）
    urdf_path: str | None = None      # PyBullet/SAPIEN (URDF）
    mjcf_path: str | None = None      # MuJoCo (MJCF）
    mjx_path: str | None = None       # MuJoCo X (MJX）

    # 物体位置
    positions: list[tuple[float, float, float]] | None = None
    default_position: tuple[float, float, float] | None = None

    # 物体旋转（四元数[w,x,y,z]）
    quat: tuple[float, float, float, float] | None = None

    # 缩放比例
    scale: tuple[float, float, float] | None = None

    # 文件类型映射
    file_type: dict[str, str] = _DEFAULT_FILE_TYPE.copy()
```

#### 具体场景配置类

```python
# roboverse_pack/scenes/kujiale_scene_0003_cfg.py

from .base_scene_cfg import SceneCfg

@configclass
class KujialeScene0003Cfg(SceneCfg):
    """Config class for Kujiale scene 0003."""

    name: str = "kujiale_0003"
    usd_path: str = "third_party/InteriorAgent/kujiale_0003/003.usda"
    positions: list[tuple[float, float, float]] = [
        (2.0, 1.8, 0.0),
    ]
    default_position: tuple[float, float, float] = (2.0, 1.8, 0.0)
    quat: tuple[float, float, float, float] = (0.0, 0.0, 0.0, 1.0)
    scale: tuple[float, float, float] = (1.0, 1.0, 1.0)
```

---

## 二、任务定义与场景引用

### 2.1 任务基类 (RLTaskEnv)

```python
# roboverse_pack/tasks/task_template.py (注意：实际在各个任务目录的base.py中）

from metasim.task.base import BaseTaskEnv
from metasim.task.rl_task import RLTaskEnv
from metasim.scenario.scenario import ScenarioCfg, SimParamCfg

class PickPlaceBase(RLTaskEnv):
    """Abstract base class for pick and place tasks."""

    def __init__(self, scenario, device=None):
        # 场景参数
        self.robot_name = self.scenario.robots[0].name
        self._action_scale = 0.04
        self.num_envs = scenario.num_envs

        # 轨迹跟踪初始化
        self._pre_init_trajectory_tracking(scenario, device)
        self._complete_trajectory_tracking_init(device)

        super().__init__(scenario, device)

        # 奖励函数
        self.reward_functions = [
            self._reward_gripper_approach,
            self._reward_gripper_close,
            self._reward_robot_target_qpos,
            self._reward_trajectory_tracking,
            self._reward_rotation_tracking,
        ]

        # 奖励权重
        self.reward_weights = [
            2.0,  # gripper_approach
            0.4,  # gripper_close
            0.1,  # robot_target_qpos
            1.0,  # trajectory_tracking
            1.0,  # rotation_tracking
        ]

    def _pre_init_trajectory_tracking(self, scenario, device):
        """Pre-initialize trajectory tracking before super().__init__()."""
        traj_config = {
            "num_waypoints": 5,
            "reach_threshold": 0.05,
            "grasp_check_distance": 0.02,
            "enable_rotation_tracking": False,
            "rotation_error_threshold": 0.1,
        }

        self.num_waypoints = traj_config["num_waypoints"]
        self.reach_threshold = traj_config["reach_threshold"]
        self.grasp_check_distance = traj_config["grasp_check_distance"]

        # 初始化轨迹跟踪状态
        self.current_waypoint_idx = torch.zeros(self._traj_num_envs, dtype=torch.long)
        self.waypoints_reached = torch.zeros(self._traj_num_envs, self.num_waypoints)
        self.object_grasped = torch.zeros(self._traj_num_envs, dtype=torch.bool)
```

### 2.2 场景配置类 (ScenarioCfg)

```python
from metasim.utils.configclass import configclass

@configclass
class ScenarioCfg:
    """Scenario configuration combining robots, objects, and simulation parameters."""

    # 机器人配置
    robots: list[SimParamCfg]

    # 物体配置
    objects: list[PrimitiveCubeCfg | RigidObjCfg]

    # 场景配置
    scene: list[str]  # 引用场景配置类名

    # 环境数量
    num_envs: int

    # 其他参数
    # (相机、奖励、随机化等）
```

### 2.3 任务与场景映射流程

```
┌──────────────────────────────────────────────────────────────────────┐
│ 1. 配置文件 (YAML)                                        │
│  roboverse_learn/rl/fast_td3/configs/pick_place_spoon2.yaml    │
│  - sim: "isaacgym"                                      │
│  - robots: ["franka"]                                    │
│  - task: "pick_place.approach_grasp_simple_spoon2"           │
│  - scene: "KujialeScene0003"                             │
└──────────────────────────────────────────────────────────────────────┘
                        │ 字符串匹配到场景配置类
                        ▼
┌──────────────────────────────────────────────────────────────────────┐
│ 2. 场景配置类 (Python)                                    │
│  roboverse_pack/scenes/kujiale_scene_0003_cfg.py              │
│  class KujialeScene0003Cfg(SceneCfg):                       │
│    name: "kujiale_0003"                                    │
│    usd_path: "third_party/InteriorAgent/kujiale_0003/003.usda"  │
│    positions: [(2.0, 1.8, 0.0)]                        │
│    quat: (0.0, 0.0, 0.0, 1.0)                          │
└──────────────────────────────────────────────────────────────────────┘
                        │ 通过元仿真框架加载
                        ▼
┌──────────────────────────────────────────────────────────────────────┐
│ 3. 任务环境 (Task Environment)                            │
│  roboverse_pack/tasks/pick_place/base.py                        │
│  class PickPlaceSpoon2(PickPlaceBase):                     │
│    - 加载场景USD文件                                          │
│    - 初始化物体和机器人                                      │
│    - 定义奖励函数和终止条件                                  │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 三、配置文件详解

### 3.1 配置文件结构

```yaml
# roboverse_learn/rl/fast_td3/configs/pick_place_spoon2.yaml

# ====================================================================
# 环境配置
# ====================================================================
sim: "isaacgym"                      # 仿真器类型
robots: ["franka"]                   # 机器人列表
task: "pick_place.approach_grasp_simple_spoon2"  # 任务类名

# ====================================================================
# 种子与设备
# ====================================================================
seed: 1
cuda: true
torch_deterministic: true
device_rank: 0

# ====================================================================
# 训练参数
# ====================================================================
num_envs: 400                         # 并行环境数量
num_eval_envs: 400                   # 评测环境数量
total_timesteps: 2000000           # 总训练步数
learning_starts: 10                    # 学习起点数量
num_steps: 1

# ====================================================================
# Replay Buffer & Batching
# ====================================================================
buffer_size: 20480                   # 回放缓冲区大小
batch_size: 32768                   # 批次大小
gamma: 0.99
tau: 0.1

# ====================================================================
# 优化器
# ====================================================================
policy_frequency: 2                    # 策略更新频率
num_updates: 5
critic_learning_rate: 0.0003
actor_learning_rate: 0.0003
weight_decay: 0.1
critic_hidden_dim: 512
actor_hidden_dim: 256
init_scale: 0.01
num_atoms: 101

# ====================================================================
# 探索
# ====================================================================
v_min: 0
v_max: 600.0
policy_noise: 0.001
std_min: 0.001
std_max: 0.4
noise_clip: 0.5

# ====================================================================
# 算法标志
# ====================================================================
use_cdq: true
compile: true
obs_normalization: true
max_grad_norm: 0.0
amp: true
amp_dtype: "fp16"
disable_bootstrap: false
measure_burnin: 3

# ====================================================================
# 日志与检查点
# ====================================================================
wandb_project: "get_started_fttd3"
exp_name: "get_started_fttd3_spoon2"
use_wandb: false
checkpoint_path: null
run_name: "pick_place.approach_grasp_simple_spoon2"
model_dir: "models/spoon2"
eval_interval: 5000
save_interval: 5000
video_width: 1024
video_height: 1024
```

### 3.2 场景引用机制

```python
# 在任务类初始化时，MetaSim框架会：

# 1. 读取配置文件获取场景类名
scene_class_name = "KujialeScene0003"

# 2. 动态导入场景配置类
from metasim.utils.configclass import configclass
from roboverse_pack.scenes.kujiale_scene_0003_cfg import KujialeScene0003Cfg

# 3. 实例化场景配置
scene_config = KujialeScene0003Cfg()

# 4. 加载场景文件（根据sim类型选择）
#    - isaacgym → 加载USD文件
#    - pybullet → 加载URDF文件
#    - mujoco → 加载MJCF文件
#    - mujoco → 加载MJX文件

# 5. 创建仿真场景
#    - 添加机器人（根据robots配置）
#    - 添加物体（根据场景positions）
#    - 设置相机、光照等
```

---

## 四、任务类型与示例

### 4.1 Pick & Place任务

**目录**：`roboverse_pack/tasks/pick_place/`

**任务示例**：

```python
class PickPlaceSpoon2(PickPlaceBase):
    """Spoon pick and place task with second scene layout."""

    def _get_initial_states(self) -> list[dict]:
        """Define initial states for the task."""

        states = []

        # 场景中所有物体的初始状态
        for env_idx in range(self.num_envs):
            # 物体（spoon）位置
            states.append({
                "object": {
                    "pos": [0.201373, -0.330642, 0.779824],
                    "rot": [-0.398238, 0.035423, -0.027580, -0.916183],
                },
                # 轨迹标记（用于waypoint跟踪）
                "traj_marker_0": {
                    "pos": [0.201373, -0.330642, 0.779824],
                    "rot": [-0.398238, 0.035423, -0.027580, -0.916183],
                },
                "traj_marker_1": {
                    "pos": [0.118386, -0.429724, 0.779824],
                    "rot": [-0.398238, 0.035423, -0.027580, -0.916183],
                },
                # ... 更多waypoints
                "traj_marker_4": {
                    "pos": [0.118386, -0.429724, 0.779824],
                    "rot": [-0.398238, 0.035423, -0.027580, -0.916183],
                },
            })

        return states
```

### 4.2 轨迹跟踪机制

```python
# 初始化轨迹跟踪
def _complete_trajectory_tracking_init(self, device):
    """Complete trajectory tracking initialization."""

    # 从initial states提取waypoint位置
    first_env_state = initial_states_list[0]

    waypoint_positions = []
    waypoint_rotations = []

    for i in range(self.num_waypoints):  # 默认5个waypoints
        marker_name = f"traj_marker_{i}"

        # 提取每个waypoint的位置和旋转
        if marker_name in first_env_state["objects"]:
            pos = first_env_state["objects"][marker_name]["pos"]
            rot = first_env_state["objects"][marker_name]["rot"]
            waypoint_positions.append(pos)
            waypoint_rotations.append(rot)

    # 转换为PyTorch张量
    self.waypoint_positions = torch.stack(waypoint_positions).to(device)
    self.waypoint_rotations = torch.stack(waypoint_rotations).to(device)
```

### 4.3 两阶段训练流程

#### 阶段1：Approach & Grasp

训练策略执行接近和抓取：

```bash
python -m roboverse_learn.rl.fast_td3.train \
    --config pick_place_spoon2.yaml
```

生成检查点：`models/spoon2/approach_grasp_simple_spoon2_*.pt`

#### 阶段2：评估Lift并收集状态

```bash
python -m roboverse_learn.rl.fast_td3.evaluate_lift \
    --checkpoint models/spoon2/approach_grasp_simple_spoon2_*.pt \
    --target_count 100 \
    --state_dir eval_states \
    --traj_dir eval_trajs
```

生成：
- **States文件**：`eval_states/pick_place.approach_grasp_simple_spoon2_lift_states_*.pkl`
  - 稳定抓取状态（成功抓取后保持稳定多帧）
  - 用于Track任务的初始状态

- **Trajectories**：`eval_trajs/*.pkl`
  - 前半段轨迹（从初始到抓取后的稳定状态）

#### 阶段3：Train Track

加载收集的状态作为初始状态：

```yaml
# track.yaml

state_file_path: "eval_states/pick_place.approach_grasp_simple_spoon2_lift_states_101states_20251122_180651.pkl"
```

训练轨迹跟踪：

```bash
python -m roboverse_learn.rl.fast_td3.train --config track.yaml
```

生成检查点：`models/spoon2/track_*.pt`

#### 阶段4：评估Track

```bash
python -m roboverse_learn.rl.fast_td3.evaluate \
    --checkpoint models/spoon2/track_*.pt
```

生成：
- **Trajectories**：后半段轨迹（从抓取后到目标位置）

#### 阶段5：合并轨迹

组合前半段和后半段轨迹：

```bash
# 使用自定义脚本或合并逻辑
# 输出：完整的pick_and_place轨迹
```

---

## 五、多场景支持

### 5.1 场景变体

**场景名称映射**：

| 场景类名 | 配置文件 | 特点 |
|-----------|---------|------|
| **KujialeScene0003** | spoon2.yaml | Scene 1布局 |
| **KujialeScene0004** | spoon2.yaml（同配置不同scene） | Scene 2布局 |
| **KujialeScene0008** | spoon2.yaml | Scene 3布局 |
| **LiberoKitchenTabletopCfg** | pick_place_bowl.yaml | 厨房桌面任务 |
| **ManycoreScene827313Cfg** | manycore场景 | 复杂桌面任务 |
| **TapwaterScene131Cfg** | track_bowl.yaml | 水龙头任务 |

### 5.2 场景配置切换

```yaml
# 同一任务配置文件可以指向不同场景

# pick_place_spoon2.yaml (Scene 1)
scene: "KujialeScene0003"
model_dir: "models/spoon2"

# 同一任务的另一个配置文件
scene: "KujialeScene0004"  # Scene 2布局
model_dir: "models/spoon2_scene4"  # 使用不同的模型目录
```

---

## 六、数据集集成

### 6.1 集成数据集来源

RoboVerse整合了多个数据集：

| 数据集 | 集成方式 | 任务类型 |
|--------|---------|---------|
| **RLBench** | 通过roboverse_pack/tasks/rlbench/ | Pick、Place等基础任务 |
| **ManiSkill** | 通过metasim集成 | 操作技能任务 |
| **LIBERO** | 通过场景配置 | 厨房桌面任务 |
| **Meta-World** | 通过roboverse_pack/tasks/mujoco_playground/ | 操作世界任务 |
| **CALVIN** | 通过roboverse_pack/tasks/calvin/ | 长视野任务 |
| **ARNetOLD** | 通过roboverse_pack/tasks/arnold/ | 抓取网络任务 |
| **GAPartNet/GAPartManip** | 通过roboverse_pack/tasks/ | 部件操作任务 |

### 6.2 机器人与仿真器支持

**机器人**：
- Franka（7-DoF机械臂）
- H1（人形机器人）
- Unitree（轮式机器人）

**仿真器**：
- Isaac Sim（通过IsaacGym）
- PyBullet
- SAPIEN 2/3
- MuJoCo
- MuJoCo X
- Genesis
- PyRep
- Blender

---

## 七、映射机制特点总结

### 7.1 核心差异：RoboTwin vs RoboVerse

| 维度 | RoboTwin | RoboVerse |
|------|-----------|-----------|
| **架构基础** | 直接Python类继承 | MetaSim框架+配置类 |
| **场景定义** | JSON指令模板 | Python场景配置类 |
| **映射方式** | scene_info.json动态存储 | YAML配置文件静态引用 |
| **任务组织** | 每个任务一个Python文件 | 分层任务（base.py + 特定任务） |
| **数据流** | info字典→指令 | config→scenario→environment |
| **语言条件** | 是（指令多样性） | 否（主要是轨迹跟踪） |
| **训练流程** | 单阶段数据收集 | 两阶段（approach+grasp + track） |

### 7.2 RoboVerse映射机制优势

1. **模块化设计**：场景配置类独立于任务，易于复用
2. **多仿真器支持**：通过file_type映射支持USD/URDF/MJCF/MJX
3. **两阶段训练**：分离approach&grasp和tracking，降低学习难度
4. **轨迹跟踪**：内置waypoint跟踪机制，支持复杂任务分解
5. **状态保存**：支持stable states收集，用于第二阶段训练
6. **多场景支持**：同一任务可在不同场景布局下训练
7. **配置驱动**：所有参数通过YAML配置，无需修改代码

### 7.3 映射流程

```
┌─────────────────────────────────────────────────────────────┐
│ 配置阶段                              │
│ - 选择task class (任务名称）                   │
│ - 选择scene class (场景配置）                 │
│ - 选择robots (机器人）                         │
│ - 设置training parameters（超参数）            │
└─────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│ 场景加载阶段                            │
│ - 动态导入scene配置类                          │
│ - 根据sim类型选择文件路径（USD/URDF等） │
│ - 加载3D场景文件                                │
│ - 添加机器人、物体、标记waypoints           │
└─────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│ 任务初始化阶段                          │
│ - 继承任务基类                                │
│ - 加载场景（通过MetaSim框架）                    │
│ - 初始化奖励函数                              │
│ - 设置轨迹跟踪状态                              │
│ - 准备initial states                            │
└─────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────┐
│ 训练/评测阶段                            │
│ - Approach & Grasp: 训练接近和抓取              │
│ - Evaluate Lift: 收集稳定状态                  │
│ - Train Track: 训练轨迹跟踪                    │
│ - Evaluate Track: 收集后半段轨迹                │
│ - Merge: 合并完整轨迹                           │
└─────────────────────────────────────────────────────────────┘
```

---

## 八、总结

RoboVerse的任务与场景映射机制采用**配置驱动+元仿真框架**的设计：

1. **场景配置类**：继承SceneCfg，定义场景名称、文件路径、物体位置
2. **配置文件映射**：YAML通过scene字段引用场景类名
3. **动态导入**：MetaSim框架根据配置动态加载场景配置
4. **分层任务**：base.py定义通用逻辑，特定任务实现细节
5. **两阶段训练**：分离approach&grasp和tracking，降低学习复杂度
6. **轨迹跟踪**：内置waypoint机制，支持复杂操作任务

与RoboTwin的**三层映射**（任务→scene_info→指令）不同，RoboVerse采用**配置类映射**（config→scene→environment），更加模块化和可扩展。
